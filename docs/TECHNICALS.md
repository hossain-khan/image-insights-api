
I want to brainstorm an idea for a REST best API service. Basically, the rest API will take in JPEG or PNG image and do analysis on the image and provide how dark the image is on scale of 0-100. Zero being black (very dark) and 100 being white. How can I design this REST API using docker container so that the micro service can be deployed anywhere by anyone.

---

## 1ï¸âƒ£ Define the problem clearly (API contract first)

You want a **stateless REST microservice** that:

* Accepts an image (JPEG / PNG)
* Computes **perceived brightness**
* Returns a **0â€“100 score**

  * `0` â†’ fully black
  * `100` â†’ fully white

### Key design decisions

* **Sync request** (fast, no queue needed)
* **No auth** (can be added later)
* **No storage** (process in memory)
* **Deterministic output**

---

## 2ï¸âƒ£ Brightness algorithm (important!)

You **should not** just average RGB values â€” human perception weights green more.

Use **luminance (Rec. 709)**:

```text
luminance = 0.2126 * R + 0.7152 * G + 0.0722 * B
```

* Range: `0â€“255`
* Normalize to `0â€“100`

```text
brightness_score = round((avg_luminance / 255) * 100)
```

This gives consistent, perceptually correct results.

---

## 3ï¸âƒ£ REST API design

### Endpoint

**Upload-based analysis:**
```
POST /v1/image/analysis
```

**URL-based analysis:**
```
POST /v1/image/analysis/url
```

### Request (Upload)

* `Content-Type: multipart/form-data`
* Field name: `image`

Example:

```bash
curl -X POST http://localhost:8080/v1/image/analysis \
  -F "image=@photo.png"
```

### Request (URL)

* `Content-Type: application/json`
* Body: `{"url": "https://example.com/image.jpg"}`

Example:

```bash
curl -X POST http://localhost:8080/v1/image/analysis/url \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/photo.jpg"}'
```

### Response (JSON)

```json
{
  "brightness_score": 73,
  "average_luminance": 186.3,
  "width": 1920,
  "height": 1080,
  "algorithm": "rec709",
  "processing_time_ms": 45.23
}
```

### Error cases

| HTTP | Reason             |
| ---- | ------------------ |
| 400  | Missing image      |
| 415  | Unsupported format |
| 413  | File too large     |
| 500  | Processing error   |

---

## 4ï¸âƒ£ Tech stack (simple + portable)

**Recommended**

* Language: **Python**
* Framework: **FastAPI**
* Image lib: **Pillow**
* Server: **Uvicorn**

Why?

* Tiny code
* Very fast for this workload
* Easy Docker story
* Excellent OpenAPI docs for free

---

## 5ï¸âƒ£ Minimal FastAPI implementation

### `main.py`

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from PIL import Image
import io
import numpy as np

app = FastAPI(title="Image Insights API")

@app.post("/v1/image/analysis")
async def analyze_brightness(image: UploadFile = File(...)):
    if image.content_type not in ["image/jpeg", "image/png"]:
        raise HTTPException(status_code=415, detail="Unsupported image type")

    try:
        contents = await image.read()
        img = Image.open(io.BytesIO(contents)).convert("RGB")
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid image")

    arr = np.array(img)

    # Rec. 709 luminance
    luminance = (
        0.2126 * arr[:, :, 0] +
        0.7152 * arr[:, :, 1] +
        0.0722 * arr[:, :, 2]
    )

    avg_luminance = float(luminance.mean())
    brightness_score = round((avg_luminance / 255) * 100)

    return {
        "brightness_score": brightness_score,
        "average_luminance": round(avg_luminance, 2),
        "width": img.width,
        "height": img.height,
        "algorithm": "rec709"
    }
```

---

## 6ï¸âƒ£ Dockerize it (this is the key portability part)

### `requirements.txt`

```
fastapi
uvicorn
pillow
numpy
```

### `Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    libjpeg62-turbo-dev \
    zlib1g-dev \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

EXPOSE 8080

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

### Build & run

```bash
docker build -t image-insights-api .
docker run -p 8080:8080 image-insights-api
```

Boom ðŸ’¥ â€” works anywhere Docker runs.

---

## 7ï¸âƒ£ OpenAPI & self-documentation (free win)

FastAPI automatically exposes:

* Swagger UI â†’ `http://localhost:8080/docs`
* OpenAPI JSON â†’ `/openapi.json`

This makes your API instantly:

* Testable
* Shareable
* SDK-generatable

---

## 8ï¸âƒ£ Deployment anywhere

Because itâ€™s **stateless + Docker**, you can deploy to:

* AWS ECS / Fargate
* Google Cloud Run
* Azure Container Apps
* Fly.io
* Kubernetes
* Even a Raspberry Pi

No config changes needed.

---

## 9ï¸âƒ£ Optional but smart improvements

### ðŸ§  Accuracy

* Downscale large images (e.g. max 512px) for speed
* Optionally return:

  * median luminance
  * histogram buckets

### ðŸ›¡ Safety

* Max file size (e.g. 5MB)
* Timeout protection

### ðŸ“ˆ Performance

* Use `--workers 2` in Uvicorn
* Vectorized NumPy is already fast

### ðŸ”Œ Extensibility

Add query params later:

```
/v1/image/analysis?metrics=brightness,median,histogram&edge_mode=left_right
```


---

### ðŸ§  Accuracy

#### âœ… Image downscaling (performance + consistency)

**Rule**

* If either width or height > `512px`, downscale
* Preserve aspect ratio
* Use high-quality resampling (LANCZOS)

**Why**

* Prevents huge memory usage
* Brightness statistics are scale-invariant
* Faster & safer

**Spec**

```
MAX_DIMENSION = 512
```

---

#### âœ… Optional extra metrics (opt-in)

Add query param:

```
POST /v1/image/analysis?include=median,histogram
```

##### Median luminance

* Compute median of luminance array
* Useful for images with highlights/shadows

##### Histogram buckets

* 10 buckets (0â€“255)
* Return normalized percentages

Example response fragment:

```json
{
  "median_luminance": 172.4,
  "histogram": [
    { "range": "0-25", "percent": 2.1 },
    { "range": "26-50", "percent": 5.3 },
    ...
    { "range": "231-255", "percent": 18.7 }
  ]
}
```

---

### ðŸ›¡ Safety

#### âœ… Max file size

* Limit: **5MB**
* Enforced **before** decoding image

**Behavior**

* Reject with `413 Payload Too Large`
* Clear error message

```json
{
  "error": "Image exceeds maximum allowed size (5MB)"
}
```

---

#### âœ… Timeout protection

* Request processing timeout: **2 seconds**
* Fail fast for pathological images

**Docker-level**

* Use Uvicorn worker timeout
* Avoid blocking calls

**API behavior**

* Return `504 Gateway Timeout`

---

### ðŸ”Œ Extensibility (important for future tools)

#### Endpoint strategy

```
/v1/image/analysis
```

**Query-driven capabilities**

```
metrics=brightness,median,histogram
method=rec709
```

Future-safe examples:

```
metrics=contrast,sharpness,blur
metrics=dominant_color
```

---

#### Internal module structure

```
app/
 â”œâ”€â”€ main.py
 â”œâ”€â”€ api/
 â”‚    â””â”€â”€ image_analysis.py
 â”œâ”€â”€ core/
 â”‚    â”œâ”€â”€ resize.py
 â”‚    â”œâ”€â”€ luminance.py
 â”‚    â”œâ”€â”€ histogram.py
 â”‚    â””â”€â”€ validators.py
 â”œâ”€â”€ config.py
```

Each metric = isolated function â†’ easy to add/remove.

---

## ðŸ”„ Caching Strategy

The API implements **LRU (Least Recently Used) + TTL (Time-to-Live) caching** to improve performance for repeated image analysis requests.

### How it works

**Cache Key**

* Generated from image file content (not filename/URL)
* Hash-based: identical images from different URLs/names return cached result
* Prevents duplicate analysis work

**Eviction Policy**

* **LRU**: When cache reaches max size, least recently used entries are removed
* **TTL**: Entries expire after configured time (default: 24 hours)
* **Hybrid**: Either eviction policy can trigger removal

### Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `CACHE_ENABLED` | `true` | Enable/disable caching |
| `CACHE_MAX_SIZE` | `512` | Maximum cached entries before LRU eviction |
| `CACHE_TTL_SECONDS` | `86400` | Expiration time in seconds (24 hours) |

### When caching helps

**Best for:**
* Repeated image analysis (e.g., mobile app re-analyzing same photo)
* Batch processing of similar images
* Development/testing with same test images
* Reduced CPU usage for frequently-analyzed images

**Not helpful for:**
* Continuous stream of unique images
* Real-time image analysis where cache invalidation is critical
* Memory-constrained environments

### When to disable caching

```bash
docker run -e CACHE_ENABLED=false image-insights-api
```

**Use cases:**
* Testing (want fresh analysis every time)
* Memory constraints (reduce RAM usage)
* Images that change frequently
* Stateless ephemeral deployments

### Performance impact

**With caching enabled:**
* Cache hit: ~1-2ms (return stored result)
* Cache miss: Normal analysis time (~20-100ms depending on image size)
* Memory: ~10-50MB for 512 entries (varies by image size)

**Example:** 100 requests, 80% cache hit rate
* Without cache: 100 Ã— 50ms = **5000ms**
* With cache: (100 Ã— 0.2 Ã— 50ms) + (100 Ã— 0.8 Ã— 1ms) = **1080ms** âœ… **5x faster**

### Sizing the cache

**For high-traffic production:**

```bash
# Increase cache size for more hits
docker run -e CACHE_MAX_SIZE=2048 image-insights-api
```

**Memory estimation:**
* Per entry: ~100-500 bytes overhead + image processing data
* 512 entries: ~50-250MB RAM
* 2048 entries: ~200-1000MB RAM

**Recommended settings:**

| Scenario | MAX_SIZE | TTL | Reasoning |
|----------|----------|-----|-----------|
| Development | 128 | 3600 (1h) | Quick testing, minimal memory |
| Small API | 512 | 86400 (24h) | Default; good for 100-1000 req/day |
| Medium API | 1024-2048 | 86400 (24h) | 1000-10,000 req/day |
| Large API | 4096+ | 14400 (4h) | 10,000+ req/day; aggressive expiry |
| Batch processing | 256 | 1800 (30m) | Short window; clear old results |

### Docker Compose example

```yaml
services:
  image-insights-api:
    image: ghcr.io/hossain-khan/image-insights-api:latest
    ports:
      - "8080:8080"
    environment:
      CACHE_ENABLED: "true"
      CACHE_MAX_SIZE: "1024"
      CACHE_TTL_SECONDS: "86400"
```

### Implementation details

**Cache storage:** In-memory Python dictionary with LRU eviction

**Thread safety:** Cache is thread-safe (Uvicorn uses multiple workers)

**Persistence:** Cache is **not persistent** across container restarts (in-memory only)
* This is intentional for stateless deployments
* Enables easy horizontal scaling
* Aligns with privacy-first design (no disk storage)

---
